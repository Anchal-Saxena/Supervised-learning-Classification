{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/hamspam123/Hamspam.csv\",encoding = \"ISO-8859-1\")","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.329037Z","iopub.execute_input":"2021-12-13T06:27:13.329674Z","iopub.status.idle":"2021-12-13T06:27:13.365485Z","shell.execute_reply.started":"2021-12-13T06:27:13.329638Z","shell.execute_reply":"2021-12-13T06:27:13.364684Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.367017Z","iopub.execute_input":"2021-12-13T06:27:13.367510Z","iopub.status.idle":"2021-12-13T06:27:13.386419Z","shell.execute_reply.started":"2021-12-13T06:27:13.367480Z","shell.execute_reply":"2021-12-13T06:27:13.385662Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.387667Z","iopub.execute_input":"2021-12-13T06:27:13.388224Z","iopub.status.idle":"2021-12-13T06:27:13.398637Z","shell.execute_reply.started":"2021-12-13T06:27:13.388189Z","shell.execute_reply":"2021-12-13T06:27:13.397911Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.510400Z","iopub.execute_input":"2021-12-13T06:27:13.510994Z","iopub.status.idle":"2021-12-13T06:27:13.525606Z","shell.execute_reply.started":"2021-12-13T06:27:13.510951Z","shell.execute_reply":"2021-12-13T06:27:13.524802Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"import re #regular expression\nimport string\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n    text = re.sub('[‘’“”…]', '', text)\n    return text\n\nclean = lambda x: clean_text(x)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.527025Z","iopub.execute_input":"2021-12-13T06:27:13.527452Z","iopub.status.idle":"2021-12-13T06:27:13.540391Z","shell.execute_reply.started":"2021-12-13T06:27:13.527413Z","shell.execute_reply":"2021-12-13T06:27:13.539664Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data['text'] = data.text.apply(clean)\ndata.text","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.541753Z","iopub.execute_input":"2021-12-13T06:27:13.542243Z","iopub.status.idle":"2021-12-13T06:27:13.686966Z","shell.execute_reply.started":"2021-12-13T06:27:13.542209Z","shell.execute_reply":"2021-12-13T06:27:13.685789Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Word frequency\nfreq = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\nfreq","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.688224Z","iopub.execute_input":"2021-12-13T06:27:13.688546Z","iopub.status.idle":"2021-12-13T06:27:13.732327Z","shell.execute_reply.started":"2021-12-13T06:27:13.688494Z","shell.execute_reply":"2021-12-13T06:27:13.731450Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#removing stopwords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:13.735086Z","iopub.execute_input":"2021-12-13T06:27:13.735349Z","iopub.status.idle":"2021-12-13T06:27:15.516002Z","shell.execute_reply.started":"2021-12-13T06:27:13.735324Z","shell.execute_reply":"2021-12-13T06:27:15.514957Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#word frequency after removal of stopwords\nfreq_Sw = pd.Series(' '.join(data['text']).split()).value_counts()[:20] # for top 20\nfreq_Sw","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:15.518856Z","iopub.execute_input":"2021-12-13T06:27:15.519161Z","iopub.status.idle":"2021-12-13T06:27:15.553163Z","shell.execute_reply.started":"2021-12-13T06:27:15.519134Z","shell.execute_reply":"2021-12-13T06:27:15.551874Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# count vectoriser tells the frequency of a word.\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nvectorizer = CountVectorizer(min_df = 1, max_df = 0.9)\nX = vectorizer.fit_transform(data[\"text\"])\nword_freq_df = pd.DataFrame({'term': vectorizer.get_feature_names(), 'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})\nword_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])\n#print(word_freq_df.sort('occurrences',ascending = False).head())","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:15.555739Z","iopub.execute_input":"2021-12-13T06:27:15.556174Z","iopub.status.idle":"2021-12-13T06:27:15.666135Z","shell.execute_reply.started":"2021-12-13T06:27:15.556130Z","shell.execute_reply":"2021-12-13T06:27:15.665379Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"word_freq_df.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:15.667372Z","iopub.execute_input":"2021-12-13T06:27:15.667947Z","iopub.status.idle":"2021-12-13T06:27:15.682198Z","shell.execute_reply.started":"2021-12-13T06:27:15.667905Z","shell.execute_reply":"2021-12-13T06:27:15.681183Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#TFIDF - Term frequency inverse Document Frequencyt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) #keep top 1000 words\ndoc_vec = vectorizer.fit_transform(data[\"text\"])\nnames_features = vectorizer.get_feature_names()\ndense = doc_vec.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns = names_features)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:15.683862Z","iopub.execute_input":"2021-12-13T06:27:15.684347Z","iopub.status.idle":"2021-12-13T06:27:17.372916Z","shell.execute_reply.started":"2021-12-13T06:27:15.684314Z","shell.execute_reply":"2021-12-13T06:27:17.371824Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:17.374510Z","iopub.execute_input":"2021-12-13T06:27:17.374961Z","iopub.status.idle":"2021-12-13T06:27:17.409802Z","shell.execute_reply.started":"2021-12-13T06:27:17.374918Z","shell.execute_reply":"2021-12-13T06:27:17.408851Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# N-gram","metadata":{}},{"cell_type":"code","source":"#Bi-gram\ndef get_top_n2_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  #for tri-gram, put ngram_range=(3,3)\n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:17.411055Z","iopub.execute_input":"2021-12-13T06:27:17.411333Z","iopub.status.idle":"2021-12-13T06:27:17.418235Z","shell.execute_reply.started":"2021-12-13T06:27:17.411306Z","shell.execute_reply":"2021-12-13T06:27:17.417380Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"top2_words = get_top_n2_words(data[\"text\"], n=200) #top 200\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\ntop2_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:17.419176Z","iopub.execute_input":"2021-12-13T06:27:17.419420Z","iopub.status.idle":"2021-12-13T06:27:17.661755Z","shell.execute_reply.started":"2021-12-13T06:27:17.419396Z","shell.execute_reply":"2021-12-13T06:27:17.660841Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Bi-gram plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntop20_bigram = top2_df.iloc[0:20,:]\nfig = plt.figure(figsize = (10, 5))\nplot=sns.barplot(x=top20_bigram[\"Bi-gram\"],y=top20_bigram[\"Freq\"])\nplot.set_xticklabels(rotation=45,labels = top20_bigram[\"Bi-gram\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:17.663103Z","iopub.execute_input":"2021-12-13T06:27:17.663359Z","iopub.status.idle":"2021-12-13T06:27:18.042562Z","shell.execute_reply.started":"2021-12-13T06:27:17.663334Z","shell.execute_reply":"2021-12-13T06:27:18.041516Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Tri-gram\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.043955Z","iopub.execute_input":"2021-12-13T06:27:18.044251Z","iopub.status.idle":"2021-12-13T06:27:18.050890Z","shell.execute_reply.started":"2021-12-13T06:27:18.044221Z","shell.execute_reply":"2021-12-13T06:27:18.049828Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"top3_words = get_top_n3_words(data[\"text\"], n=200)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.052315Z","iopub.execute_input":"2021-12-13T06:27:18.052637Z","iopub.status.idle":"2021-12-13T06:27:18.283642Z","shell.execute_reply.started":"2021-12-13T06:27:18.052596Z","shell.execute_reply":"2021-12-13T06:27:18.282461Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"top3_df","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.284879Z","iopub.execute_input":"2021-12-13T06:27:18.285166Z","iopub.status.idle":"2021-12-13T06:27:18.297775Z","shell.execute_reply.started":"2021-12-13T06:27:18.285140Z","shell.execute_reply":"2021-12-13T06:27:18.296863Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#Tri-gram plot\nimport seaborn as sns\ntop20_trigram = top3_df.iloc[0:20,:]\nfig = plt.figure(figsize = (10, 5))\nplot=sns.barplot(x=top20_trigram[\"Tri-gram\"],y=top20_trigram[\"Freq\"])\nplot.set_xticklabels(rotation=45,labels = top20_trigram[\"Tri-gram\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.299725Z","iopub.execute_input":"2021-12-13T06:27:18.300077Z","iopub.status.idle":"2021-12-13T06:27:18.563476Z","shell.execute_reply.started":"2021-12-13T06:27:18.300035Z","shell.execute_reply":"2021-12-13T06:27:18.562669Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# WordCloud","metadata":{}},{"cell_type":"code","source":"string_Total = \" \".join(data[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.564604Z","iopub.execute_input":"2021-12-13T06:27:18.565077Z","iopub.status.idle":"2021-12-13T06:27:18.569651Z","shell.execute_reply.started":"2021-12-13T06:27:18.564984Z","shell.execute_reply":"2021-12-13T06:27:18.568885Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#wordcloud for entire corpus\nfrom wordcloud import WordCloud\nwordcloud_stw = WordCloud(\n                background_color= 'black',\n                width = 1800,\n                height = 1500\n                ).generate(string_Total)\nplt.imshow(wordcloud_stw)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:18.571083Z","iopub.execute_input":"2021-12-13T06:27:18.571414Z","iopub.status.idle":"2021-12-13T06:27:26.656963Z","shell.execute_reply.started":"2021-12-13T06:27:18.571376Z","shell.execute_reply":"2021-12-13T06:27:26.655937Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# Applying naive bayes for classification","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.658416Z","iopub.execute_input":"2021-12-13T06:27:26.659208Z","iopub.status.idle":"2021-12-13T06:27:26.672360Z","shell.execute_reply.started":"2021-12-13T06:27:26.659160Z","shell.execute_reply":"2021-12-13T06:27:26.671137Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def split_into_words(i):\n    return (i.split(\" \"))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.673827Z","iopub.execute_input":"2021-12-13T06:27:26.674382Z","iopub.status.idle":"2021-12-13T06:27:26.684917Z","shell.execute_reply.started":"2021-12-13T06:27:26.674341Z","shell.execute_reply":"2021-12-13T06:27:26.684009Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nemail_train,email_test = train_test_split(data,test_size=0.3)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.686165Z","iopub.execute_input":"2021-12-13T06:27:26.686721Z","iopub.status.idle":"2021-12-13T06:27:26.700883Z","shell.execute_reply.started":"2021-12-13T06:27:26.686678Z","shell.execute_reply":"2021-12-13T06:27:26.699981Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"email_test","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.704496Z","iopub.execute_input":"2021-12-13T06:27:26.705713Z","iopub.status.idle":"2021-12-13T06:27:26.722761Z","shell.execute_reply.started":"2021-12-13T06:27:26.705660Z","shell.execute_reply":"2021-12-13T06:27:26.721808Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Preparing email texts into word count matrix format \nemails_bow = CountVectorizer(analyzer=split_into_words).fit(data.text)\n\n# [\"mailing\",\"body\",\"texting\"]\n# [\"mailing\",\"awesome\",\"good\"]\n\n# [\"mailing\",\"body\",\"texting\",\"good\",\"awesome\"]\n\n\n\n#        \"mailing\" \"body\" \"texting\" \"good\" \"awesome\"\n#  0          1        1       1        0       0\n \n#  1          1        0        0       1       1    \n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.724334Z","iopub.execute_input":"2021-12-13T06:27:26.725081Z","iopub.status.idle":"2021-12-13T06:27:26.784578Z","shell.execute_reply.started":"2021-12-13T06:27:26.725034Z","shell.execute_reply":"2021-12-13T06:27:26.783213Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For all messages\nall_emails_matrix = emails_bow.transform(data.text)\nall_emails_matrix.shape ","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.788528Z","iopub.execute_input":"2021-12-13T06:27:26.788910Z","iopub.status.idle":"2021-12-13T06:27:26.835323Z","shell.execute_reply.started":"2021-12-13T06:27:26.788877Z","shell.execute_reply":"2021-12-13T06:27:26.834159Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# For training messages\ntrain_emails_matrix = emails_bow.transform(email_train.text)\ntrain_emails_matrix.shape # (3891,8175)\n\n# For testing messages\ntest_emails_matrix = emails_bow.transform(email_test.text)\ntest_emails_matrix.shape # (1668,8175)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.837007Z","iopub.execute_input":"2021-12-13T06:27:26.837613Z","iopub.status.idle":"2021-12-13T06:27:26.887744Z","shell.execute_reply.started":"2021-12-13T06:27:26.837562Z","shell.execute_reply":"2021-12-13T06:27:26.886676Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"####### Without TFIDF matrices ########################\n# Preparing a naive bayes model on training data set \n\nfrom sklearn.naive_bayes import MultinomialNB as MB\nfrom sklearn.naive_bayes import GaussianNB as GB\n\n# Multinomial Naive Bayes\nclassifier_mb = MB()\nclassifier_mb.fit(train_emails_matrix,email_train.type)\ntrain_pred_m = classifier_mb.predict(train_emails_matrix)\naccuracy_train_m = np.mean(train_pred_m==email_train.type) # 98%\n\ntest_pred_m = classifier_mb.predict(test_emails_matrix)\naccuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.889786Z","iopub.execute_input":"2021-12-13T06:27:26.890517Z","iopub.status.idle":"2021-12-13T06:27:26.916143Z","shell.execute_reply.started":"2021-12-13T06:27:26.890463Z","shell.execute_reply":"2021-12-13T06:27:26.915166Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"accuracy_train_m","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.920004Z","iopub.execute_input":"2021-12-13T06:27:26.920329Z","iopub.status.idle":"2021-12-13T06:27:26.928315Z","shell.execute_reply.started":"2021-12-13T06:27:26.920296Z","shell.execute_reply":"2021-12-13T06:27:26.927477Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_pred_m","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.929583Z","iopub.execute_input":"2021-12-13T06:27:26.930065Z","iopub.status.idle":"2021-12-13T06:27:26.942558Z","shell.execute_reply.started":"2021-12-13T06:27:26.930021Z","shell.execute_reply":"2021-12-13T06:27:26.941475Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes \nclassifier_gb = GB()\nclassifier_gb.fit(train_emails_matrix.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\ntrain_pred_g = classifier_gb.predict(train_emails_matrix.toarray())\naccuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\n\ntest_pred_g = classifier_gb.predict(test_emails_matrix.toarray())\naccuracy_test_g = np.mean(test_pred_g==email_test.type) # 80%","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:26.943842Z","iopub.execute_input":"2021-12-13T06:27:26.944320Z","iopub.status.idle":"2021-12-13T06:27:28.464079Z","shell.execute_reply.started":"2021-12-13T06:27:26.944290Z","shell.execute_reply":"2021-12-13T06:27:28.462863Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# Using TFIDF","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\n\n# Learning Term weighting and normalizing on entire emails\ntfidf_transformer = TfidfTransformer().fit(all_emails_matrix)\n\n# Preparing TFIDF for train emails\ntrain_tfidf = tfidf_transformer.transform(train_emails_matrix)\n\ntrain_tfidf.shape # (3891, 6661)\n\n# Preparing TFIDF for test emails\ntest_tfidf = tfidf_transformer.transform(test_emails_matrix)\n\ntest_tfidf.shape #  (1668, 6661)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:28.465349Z","iopub.execute_input":"2021-12-13T06:27:28.465646Z","iopub.status.idle":"2021-12-13T06:27:28.476464Z","shell.execute_reply.started":"2021-12-13T06:27:28.465619Z","shell.execute_reply":"2021-12-13T06:27:28.475564Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Preparing a naive bayes model on training data set \n\nfrom sklearn.naive_bayes import MultinomialNB as MB\nfrom sklearn.naive_bayes import GaussianNB as GB\n\n# Multinomial Naive Bayes\nclassifier_mb = MB()\nclassifier_mb.fit(train_tfidf,email_train.type)\ntrain_pred_m = classifier_mb.predict(train_tfidf)\naccuracy_train_m = np.mean(train_pred_m==email_train.type) # 96%\n\ntest_pred_m = classifier_mb.predict(test_tfidf)\naccuracy_test_m = np.mean(test_pred_m==email_test.type) # 96%\n\n \n","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:28.480195Z","iopub.execute_input":"2021-12-13T06:27:28.480605Z","iopub.status.idle":"2021-12-13T06:27:28.500260Z","shell.execute_reply.started":"2021-12-13T06:27:28.480574Z","shell.execute_reply":"2021-12-13T06:27:28.499390Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"accuracy_train_m","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:28.501387Z","iopub.execute_input":"2021-12-13T06:27:28.501780Z","iopub.status.idle":"2021-12-13T06:27:28.506401Z","shell.execute_reply.started":"2021-12-13T06:27:28.501752Z","shell.execute_reply":"2021-12-13T06:27:28.505625Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Gaussian Naive Bayes \nclassifier_gb = GB()\nclassifier_gb.fit(train_tfidf.toarray(),email_train.type.values) # we need to convert tfidf into array format which is compatible for gaussian naive bayes\ntrain_pred_g = classifier_gb.predict(train_tfidf.toarray())\naccuracy_train_g = np.mean(train_pred_g==email_train.type) # 95%\ntest_pred_g = classifier_gb.predict(test_tfidf.toarray())\naccuracy_test_g = np.mean(test_pred_g==email_test.type) # 88%\n\n# inplace of tfidf we can also use train_emails_matrix and test_emails_matrix instead of term inverse document frequency matrix","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:28.507749Z","iopub.execute_input":"2021-12-13T06:27:28.508016Z","iopub.status.idle":"2021-12-13T06:27:30.047106Z","shell.execute_reply.started":"2021-12-13T06:27:28.507989Z","shell.execute_reply":"2021-12-13T06:27:30.046017Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"accuracy_test_g","metadata":{"execution":{"iopub.status.busy":"2021-12-13T06:27:30.048378Z","iopub.execute_input":"2021-12-13T06:27:30.048712Z","iopub.status.idle":"2021-12-13T06:27:30.053428Z","shell.execute_reply.started":"2021-12-13T06:27:30.048679Z","shell.execute_reply":"2021-12-13T06:27:30.052742Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}